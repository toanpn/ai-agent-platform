"""
Master Agent Module

This module contains the Master Agent (Coordinator Agent) that receives all user requests
and intelligently routes them to the most appropriate Sub-Agent based on the request content
and agent descriptions.
"""

import os
import re
from typing import List, Dict, Any
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.tools import BaseTool
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI


class MasterAgent:
    def __init__(self, sub_agents_as_tools: List[BaseTool]):
        """
        Initialize the Master Agent with sub-agents as tools.
        
        Args:
            sub_agents_as_tools: List of sub-agents wrapped as BaseTool objects
        """
        self.sub_agents = self._sanitize_sub_agent_tools(sub_agents_as_tools)
        self.agent_executor = self._create_master_agent_executor()
        # Initialize LLM for comparison summaries
        self.comparison_llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            temperature=0.3
        )
    
    def _sanitize_sub_agent_tools(self, sub_agents: List[BaseTool]) -> List[BaseTool]:
        """
        Sanitizes sub-agent names to be compliant with Gemini's function naming rules.
        """
        for agent_tool in sub_agents:
            original_name = agent_tool.name
            
            # Gemini function name rules:
            # - Must start with a letter or an underscore.
            # - Must be alphanumeric (a-z, A-Z, 0-9), underscores (_), dots (.), or dashes (-).
            # - Maximum length of 64.
            
            # Replace invalid characters with an underscore
            sanitized_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', original_name)
            
            # Ensure it starts with a letter or underscore
            if not re.match(r'^[a-zA-Z_]', sanitized_name):
                sanitized_name = '_' + sanitized_name
            
            # Enforce maximum length of 64 characters
            sanitized_name = sanitized_name[:64]
            
            if sanitized_name != original_name:
                print(f"Warning: Sanitized invalid agent name from '{original_name}' to '{sanitized_name}'")
                agent_tool.name = sanitized_name
        
        return sub_agents
    
    def _detect_comparison_request(self, user_input: str) -> Dict[str, Any]:
        """
        Detect if user is asking for a comparison between business models.
        
        Args:
            user_input: The user's query
            
        Returns:
            dict: Contains is_comparison flag and list of models to compare
        """
        comparison_keywords = [
            "so s√°nh", "compare", "ch·ªçn gi·ªØa", "choose between", "kh√°c nhau", "difference", 
            "n√™n ch·ªçn", "should choose", "ph√π h·ª£p h∆°n", "more suitable", "t·ªët h∆°n", "better",
            "∆∞u nh∆∞·ª£c ƒëi·ªÉm", "pros and cons", "l·ª±a ch·ªçn", "choice", "quy·∫øt ƒë·ªãnh", "decide"
        ]
        
        model_keywords = {
            "fnb": ["fnb", "f&b", "food", "beverage", "nh√† h√†ng", "restaurant", "qu√°n ƒÉn", "m√≥n ƒÉn", "th·ª©c u·ªëng"],
            "booking": ["booking", "ƒë·∫∑t ph√≤ng", "hotel", "kh√°ch s·∫°n", "resort", "homestay", "accommodation"]
        }
        
        # Check if it's a comparison request
        is_comparison = any(keyword.lower() in user_input.lower() for keyword in comparison_keywords)
        
        # Detect which models are mentioned
        detected_models = []
        for model_type, keywords in model_keywords.items():
            if any(keyword.lower() in user_input.lower() for keyword in keywords):
                detected_models.append(model_type)
        
        return {
            "is_comparison": is_comparison and len(detected_models) >= 2,
            "models": detected_models,
            "comparison_type": "business_models" if is_comparison and detected_models else None
        }
    
    def _find_agents_by_model_type(self, model_types: List[str]) -> List[BaseTool]:
        """
        Find sub-agents that match the requested model types.
        
        Args:
            model_types: List of model types (e.g., ['fnb', 'booking'])
            
        Returns:
            List of matching agents
        """
        matching_agents = []
        
        for model_type in model_types:
            for agent in self.sub_agents:
                agent_name_lower = agent.name.lower()
                if f"sale_support_{model_type}" in agent_name_lower or f"{model_type}" in agent_name_lower:
                    matching_agents.append(agent)
                    break
        
        return matching_agents
    
    def _call_multiple_agents(self, agents: List[BaseTool], user_query: str) -> Dict[str, str]:
        """
        Call multiple agents and collect their responses.
        
        Args:
            agents: List of agents to call
            user_query: The user's question
            
        Returns:
            dict: Agent name -> response mapping
        """
        responses = {}
        
        for agent in agents:
            try:
                print(f"üîÑ Calling agent: {agent.name}")
                
                # Prepare the query for the specific agent
                agent_specific_query = f"""H√£y gi·∫£i th√≠ch v·ªÅ m√¥ h√¨nh kinh doanh m√† b·∫°n h·ªó tr·ª£, bao g·ªìm:
1. ƒê·∫∑c ƒëi·ªÉm ch√≠nh c·ªßa m√¥ h√¨nh
2. ∆Øu ƒëi·ªÉm v√† l·ª£i √≠ch
3. ƒê·ªëi t∆∞·ª£ng kh√°ch h√†ng ph√π h·ª£p
4. C√°c t√≠nh nƒÉng h·ªó tr·ª£ ch√≠nh

C√¢u h·ªèi g·ªëc t·ª´ ng∆∞·ªùi d√πng: {user_query}

Vui l√≤ng tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch chi ti·∫øt v√† c·ª• th·ªÉ."""
                
                # Call the agent
                response = agent.invoke(agent_specific_query)
                responses[agent.name] = response
                
                print(f"‚úÖ Received response from {agent.name}")
                
            except Exception as e:
                print(f"‚ùå Error calling agent {agent.name}: {str(e)}")
                responses[agent.name] = f"L·ªói khi g·ªçi agent {agent.name}: {str(e)}"
        
        return responses
    
    def _summarize_comparison_responses(self, responses: Dict[str, str], user_query: str) -> str:
        """
        Summarize and compare responses from multiple agents.
        
        Args:
            responses: Agent name -> response mapping
            user_query: Original user query
            
        Returns:
            str: Summarized comparison response
        """
        try:
            # Format responses for comparison
            formatted_responses = []
            for agent_name, response in responses.items():
                # Extract model type from agent name
                model_type = "FNB" if "fnb" in agent_name.lower() else "Booking" if "booking" in agent_name.lower() else agent_name
                formatted_responses.append(f"**M√¥ h√¨nh {model_type}:**\n{response}")
            
            combined_responses = "\n\n" + "="*50 + "\n\n".join(formatted_responses)
            
            # Create comparison prompt
            comparison_prompt = ChatPromptTemplate.from_messages([
                ("system", """B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n kinh doanh c·ªßa KiotViet. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch v√† so s√°nh c√°c m√¥ h√¨nh kinh doanh ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n ph√π h·ª£p cho kh√°ch h√†ng.

H√£y d·ª±a v√†o th√¥ng tin t·ª´ c√°c chuy√™n gia v·ªÅ t·ª´ng m√¥ h√¨nh kinh doanh ƒë·ªÉ t·∫°o ra m·ªôt b·∫£n so s√°nh chi ti·∫øt v√† khuy·∫øn ngh·ªã ph√π h·ª£p.

Y·ªÇU C·∫¶U PH·∫¢N H·ªíI:
1. T√≥m t·∫Øt ng·∫Øn g·ªçn ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng m√¥ h√¨nh
2. B·∫£ng so s√°nh ∆∞u nh∆∞·ª£c ƒëi·ªÉm
3. Khuy·∫øn ngh·ªã d·ª±a tr√™n lo·∫°i h√¨nh kinh doanh v√† quy m√¥
4. K·∫øt lu·∫≠n v√† l·ªùi khuy√™n c·ª• th·ªÉ

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c·∫•u tr√∫c r√µ r√†ng v√† d·ªÖ hi·ªÉu."""),
                ("human", """C√¢u h·ªèi t·ª´ kh√°ch h√†ng: {user_query}

Th√¥ng tin t·ª´ c√°c chuy√™n gia:
{agent_responses}

H√£y t·∫°o m·ªôt b·∫£n ph√¢n t√≠ch so s√°nh chi ti·∫øt v√† ƒë∆∞a ra khuy·∫øn ngh·ªã ph√π h·ª£p.""")
            ])
            
            # Generate comparison response
            chain = comparison_prompt | self.comparison_llm
            result = chain.invoke({
                "user_query": user_query,
                "agent_responses": combined_responses
            })
            
            return result.content
            
        except Exception as e:
            print(f"‚ùå Error in comparison summary: {str(e)}")
            
            # Fallback: Return formatted responses
            fallback_response = f"""**So s√°nh m√¥ h√¨nh kinh doanh**

D·ª±a tr√™n c√¢u h·ªèi: "{user_query}"

"""
            for agent_name, response in responses.items():
                model_type = "FNB" if "fnb" in agent_name.lower() else "Booking" if "booking" in agent_name.lower() else agent_name
                fallback_response += f"## M√¥ h√¨nh {model_type}:\n{response}\n\n"
                
            fallback_response += """**K·∫øt lu·∫≠n:** Vui l√≤ng li√™n h·ªá v·ªõi ƒë·ªôi ng≈© t∆∞ v·∫•n KiotViet ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt h∆°n trong vi·ªác l·ª±a ch·ªçn m√¥ h√¨nh kinh doanh ph√π h·ª£p."""
            
            return fallback_response
    
    def _create_master_agent_executor(self) -> AgentExecutor:
        """Creates the Master Agent executor with sub-agents as its tools."""
        
        # Create a comprehensive prompt for the master agent
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an intelligent Master Agent (Coordinator) responsible for analyzing user requests and routing them to the most appropriate specialist agent. You are the central brain of KiotViet's multi-agent system.

üè™ KIOTVIET COMPANY CONTEXT:
- KiotViet is Vietnam's leading sales management software company
- Main product: Comprehensive sales management platform for stores, restaurants, spas, clinics
- Core features: Inventory management, POS sales, revenue reporting, customer management, marketing automation
- Target customers: SMEs (Small & Medium Enterprises) in Vietnam
- Departments: Sales, Customer Service (CSKH), Dev, Test, Product (PE), HR, IT
- Company culture: Innovation, efficiency, customer-focused

üéØ CORE MISSION: Analyze, Route, Coordinate - Never answer directly, always delegate to specialists with KiotViet context.

üìã AVAILABLE SPECIALIST AGENTS:
{agent_descriptions}

üß† INTELLIGENT ROUTING DECISION FRAMEWORK:

1. **REQUEST ANALYSIS**:
   - Identify the primary domain: HR, IT/Technical, Research, or General
   - Look for key indicators and keywords
   - Consider the action type: create, search, troubleshoot, manage, etc.

2. **DOMAIN KEYWORDS MAPPING**:
   
   **HR_Agent** ‚Üí Use for:
   - Keywords: "nh√¢n s·ª±", "HR", "employee", "nh√¢n vi√™n", "ch√≠nh s√°ch", "policy", "ngh·ªâ ph√©p", "leave", "tuy·ªÉn d·ª•ng", "recruitment", "benefits", "l∆∞∆°ng", "salary", "ƒë√°nh gi√°", "performance", "onboarding", "offboarding"
   - Actions: Employee queries, policy questions, leave requests, benefits info, HR procedures
   
   **PE_Agent** ‚Üí Use for:
   - Keywords: "s·∫£n ph·∫©m", "product", "d·ª± √°n", "project", "ph√°t tri·ªÉn", "development", "business", "kinh doanh", "y√™u c·∫ßu", "requirements", "user story", "sprint", "scrum", "JIRA", "ticket", "stakeholder", "ph√¢n t√≠ch", "analysis", "documentation", "t√†i li·ªáu", "workflow", "process", "strategy", "chi·∫øn l∆∞·ª£c", "market", "th·ªã tr∆∞·ªùng", "competitor", "ƒë·ªëi th·ªß", "research", "nghi√™n c·ª©u"
   - Actions: Product development, business analysis, project management, requirements gathering, JIRA management, stakeholder communication

3. **ROUTING DECISION RULES**:
   - **Primary Rule**: Match domain keywords first
   - **HR Priority**: Use HR_Agent for any employee/policy/HR-related questions
   - **PE Priority**: Use PE_Agent for product development, business analysis, JIRA, project management
   - **Tool Consideration**: JIRA tasks ‚Üí PE_Agent, HR policies ‚Üí HR_Agent
   - **Fallback Rule**: When unclear, prefer PE_Agent as it has more comprehensive tools

4. **DELEGATION INSTRUCTIONS**:
   - Pass the COMPLETE original user question to the selected agent
   - Add Vietnamese response instruction: "Vui l√≤ng tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."
   - Include relevant context if available

5. **CRITICAL REQUIREMENTS**:
   - NEVER answer questions yourself - ALWAYS delegate
   - ALWAYS choose exactly ONE agent
   - ALWAYS pass the full user query to the selected agent
   - ALWAYS ensure Vietnamese responses from sub-agents

Example routing decisions for KiotViet:
- "T·∫°o JIRA ticket cho bug POS system" ‚Üí PE_Agent
- "Ch√≠nh s√°ch ngh·ªâ ph√©p c·ªßa KiotViet" ‚Üí HR_Agent  
- "Ph√¢n t√≠ch requirements cho t√≠nh nƒÉng inventory m·ªõi" ‚Üí PE_Agent
- "Th√¥ng tin v·ªÅ benefits nh√¢n vi√™n KiotViet" ‚Üí HR_Agent
- "Sprint planning cho KiotViet mobile app" ‚Üí PE_Agent
- "Onboarding developer m·ªõi v√†o team Dev" ‚Üí HR_Agent
- "Competitor analysis MISA vs KiotViet" ‚Üí PE_Agent
- "Quy tr√¨nh performance review" ‚Üí HR_Agent

Remember: You are a smart router, not an answerer. Trust your specialists to handle their domains of expertise!"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])
        
        # Generate agent descriptions for the prompt
        agent_descriptions = []
        for agent in self.sub_agents:
            agent_descriptions.append(f"- {agent.name}: {agent.description}")
        
        # Format the prompt with agent descriptions
        formatted_prompt = prompt.partial(agent_descriptions="\n".join(agent_descriptions))
        
        # Initialize the LLM for the master agent
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash", 
                temperature=0.1  # Low temperature for consistent routing decisions
            )
        except Exception as e:
            raise RuntimeError(f"Failed to initialize LLM for Master Agent: {e}")
        
        # Create the master agent
        agent = create_tool_calling_agent(llm, self.sub_agents, formatted_prompt)
        
        # Create the agent executor
        master_agent_executor = AgentExecutor(
            agent=agent,
            tools=self.sub_agents,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=5,
            return_intermediate_steps=True  # Help with debugging
        )
        
        return master_agent_executor
    
    def process_request(self, user_input: str) -> str:
        """
        Process a user request by routing it to the appropriate sub-agent(s).
        For comparison requests, calls multiple agents and summarizes responses.
        
        Args:
            user_input: The user's query or request
            
        Returns:
            str: The response from the appropriate sub-agent(s) or comparison summary
        """
        try:
            # Log the incoming request with analysis
            print(f"\nüéØ MASTER AGENT: Ph√¢n t√≠ch y√™u c·∫ßu t·ª´ user...")
            print(f"üìù User Input: {user_input}")
            
            # Check if this is a comparison request
            comparison_analysis = self._detect_comparison_request(user_input)
            
            if comparison_analysis["is_comparison"]:
                print(f"üîç COMPARISON REQUEST DETECTED!")
                print(f"üìä Models to compare: {comparison_analysis['models']}")
                
                # Find matching agents for comparison
                matching_agents = self._find_agents_by_model_type(comparison_analysis["models"])
                
                if len(matching_agents) >= 2:
                    print(f"‚úÖ Found {len(matching_agents)} agents for comparison: {[agent.name for agent in matching_agents]}")
                    
                    # Call multiple agents
                    agent_responses = self._call_multiple_agents(matching_agents, user_input)
                    
                    # Summarize and compare responses
                    comparison_summary = self._summarize_comparison_responses(agent_responses, user_input)
                    
                    print(f"‚úÖ MASTER AGENT: ƒê√£ t·∫°o b·∫£n so s√°nh t·ª´ {len(matching_agents)} agents")
                    return comparison_summary
                    
                else:
                    print(f"‚ö†Ô∏è Could not find enough matching agents for comparison. Found: {[agent.name for agent in matching_agents]}")
                    print(f"üìã Available agents: {[agent.name for agent in self.sub_agents]}")
                    
                    # Fallback to normal routing if not enough agents found
                    print("üîÑ Falling back to normal single-agent routing...")
            
            # Normal single-agent routing
            self._log_routing_analysis(user_input)
            
            # Process the request through the agent executor
            result = self.agent_executor.invoke({"input": user_input})
            
            # Extract the output
            output = result.get("output", "Kh√¥ng c√≥ ph·∫£n h·ªìi ƒë∆∞·ª£c t·∫°o")
            
            # Log successful routing
            print(f"‚úÖ MASTER AGENT: ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng v√† nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi")
            
            return output
            
        except Exception as e:
            error_msg = f"‚ùå Master Agent g·∫∑p l·ªói: {str(e)}"
            print(f"‚ùå Error: {error_msg}")
            
            # Provide fallback response in Vietnamese
            fallback_response = f"""Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n. 

L·ªói: {str(e)}

Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c li√™n h·ªá v·ªõi b·ªô ph·∫≠n h·ªó tr·ª£ k·ªπ thu·∫≠t n·∫øu v·∫•n ƒë·ªÅ v·∫´n ti·∫øp t·ª•c."""
            
            return fallback_response
    
    def _log_routing_analysis(self, user_input: str):
        """Log analysis for routing decision debugging."""
        print(f"\nüîç ROUTING ANALYSIS:")
        
        # Check for comparison request first
        comparison_analysis = self._detect_comparison_request(user_input)
        if comparison_analysis["is_comparison"]:
            print(f"üîÑ COMPARISON MODE: Detected request to compare {comparison_analysis['models']}")
            matching_agents = self._find_agents_by_model_type(comparison_analysis["models"])
            print(f"üéØ Target agents for comparison: {[agent.name for agent in matching_agents]}")
            return
        
        # Check for key domain indicators
        hr_keywords = ["nh√¢n s·ª±", "HR", "employee", "nh√¢n vi√™n", "ch√≠nh s√°ch", "policy", "ngh·ªâ ph√©p", "leave", "l∆∞∆°ng", "salary", "benefits", "tuy·ªÉn d·ª•ng", "recruitment", "onboarding", "offboarding", "xin ngh·ªâ vi·ªác", "ngh·ªâ vi·ªác", "resignation", "th·ªß t·ª•c"]
        pe_keywords = ["s·∫£n ph·∫©m", "product", "d·ª± √°n", "project", "ph√°t tri·ªÉn", "development", "business", "kinh doanh", "y√™u c·∫ßu", "requirements", "user story", "sprint", "scrum", "JIRA", "ticket", "stakeholder", "ph√¢n t√≠ch", "analysis", "documentation", "t√†i li·ªáu", "workflow", "process", "strategy", "chi·∫øn l∆∞·ª£c", "market", "th·ªã tr∆∞·ªùng", "competitor", "ƒë·ªëi th·ªß", "research", "nghi√™n c·ª©u"]
        
        detected_domains = []
        
        for keyword in hr_keywords:
            if keyword.lower() in user_input.lower():
                detected_domains.append(f"HR ({keyword})")
                
        for keyword in pe_keywords:
            if keyword.lower() in user_input.lower():
                detected_domains.append(f"PE ({keyword})")
        
        if detected_domains:
            print(f"üéØ Detected domains: {', '.join(detected_domains)}")
        else:
            print(f"ü§î No specific domain detected - will default to PE_Agent")
        
        print(f"üìä Available agents: {[agent.name for agent in self.sub_agents]}")
        print(f"‚ö° Starting routing process...\n")
    
    def process_request_with_details(self, user_input: str) -> dict:
        """
        Process a user request and return both response and execution details.
        Handles both single-agent routing and multi-agent comparison.
        
        Args:
            user_input: The user's query or request
            
        Returns:
            dict: Contains response, agents used, tools used, and execution details
        """
        try:
            # Log the incoming request
            print(f"Master Agent received request: {user_input}")
            
            # Check if this is a comparison request
            comparison_analysis = self._detect_comparison_request(user_input)
            
            if comparison_analysis["is_comparison"]:
                print(f"Processing comparison request for models: {comparison_analysis['models']}")
                
                # Find matching agents for comparison
                matching_agents = self._find_agents_by_model_type(comparison_analysis["models"])
                
                if len(matching_agents) >= 2:
                    # Call multiple agents for comparison
                    agent_responses = self._call_multiple_agents(matching_agents, user_input)
                    
                    # Summarize and compare responses
                    comparison_summary = self._summarize_comparison_responses(agent_responses, user_input)
                    
                    # Return comparison details
                    return {
                        "response": comparison_summary,
                        "agents_used": [agent.name for agent in matching_agents],
                        "tools_used": [agent.name for agent in matching_agents],  # Agents are also tools
                        "execution_steps": [
                            {
                                "tool_name": agent.name,
                                "tool_input": f"Comparison query about business models",
                                "observation": agent_responses.get(agent.name, "No response")
                            } for agent in matching_agents
                        ] + [
                            {
                                "tool_name": "comparison_summarizer",
                                "tool_input": "Summarize comparison responses",
                                "observation": "Comparison summary generated"
                            }
                        ],
                        "total_steps": len(matching_agents) + 1,
                        "comparison_mode": True
                    }
                else:
                    print(f"Not enough agents found for comparison, falling back to normal routing")
            
            # Normal single-agent processing
            result = self.agent_executor.invoke({"input": user_input})
            
            # Extract the output
            output = result.get("output", "No response generated")
            
            # Extract execution details
            intermediate_steps = result.get("intermediate_steps", [])
            
            # Analyze intermediate steps to extract agents and tools used
            agents_used = set()
            tools_used = set()
            execution_steps = []
            
            for step in intermediate_steps:
                if len(step) >= 2:
                    action, observation = step[0], step[1]
                    
                    # Extract tool/agent name from action
                    if hasattr(action, 'tool'):
                        tool_name = action.tool
                        tools_used.add(tool_name)
                        
                        # Check if this tool is actually a sub-agent
                        for agent in self.sub_agents:
                            if agent.name == tool_name:
                                agents_used.add(tool_name)
                                break
                        else:
                            # It's a regular tool, not a sub-agent
                            pass
                    
                    # Record execution step
                    execution_steps.append({
                        "tool_name": getattr(action, 'tool', 'unknown'),
                        "tool_input": getattr(action, 'tool_input', ''),
                        "observation": str(observation)
                    })
            
            return {
                "response": output,
                "agents_used": list(agents_used),
                "tools_used": list(tools_used),
                "execution_steps": execution_steps,
                "total_steps": len(intermediate_steps),
                "comparison_mode": False
            }
            
        except Exception as e:
            error_msg = f"Master Agent encountered an error: {str(e)}"
            print(f"Error: {error_msg}")
            return {
                "response": error_msg,
                "agents_used": [],
                "tools_used": [],
                "execution_steps": [],
                "total_steps": 0,
                "error": str(e),
                "comparison_mode": False
            }
    
    def process_request_with_details_and_history(self, user_input: str, history: List[dict]) -> dict:
        """
        Process a user request with conversation history and return both response and execution details.
        Handles both single-agent routing and multi-agent comparison with history context.
        
        Args:
            user_input: The user's current query or request
            history: List of previous conversation messages with format:
                    [{"role": "user|assistant", "content": "message", "agentName": "name", "timestamp": "..."}]
            
        Returns:
            dict: Contains response, agents used, tools used, and execution details
        """
        try:
            # Log the incoming request with history
            print(f"Master Agent received request with history: {user_input}")
            print(f"Conversation history: {len(history)} messages")
            
            # Format conversation history for context
            conversation_context = self._format_conversation_history(history)
            
            # Create context-aware input that includes conversation history
            contextual_input = f"""Conversation History:
{conversation_context}

Current User Message: {user_input}

Please respond to the current user message while taking into account the conversation history above. Maintain context and continuity from previous exchanges."""
            
            # Check if this is a comparison request (using original user input for detection)
            comparison_analysis = self._detect_comparison_request(user_input)
            
            if comparison_analysis["is_comparison"]:
                print(f"Processing comparison request with history for models: {comparison_analysis['models']}")
                
                # Find matching agents for comparison
                matching_agents = self._find_agents_by_model_type(comparison_analysis["models"])
                
                if len(matching_agents) >= 2:
                    # Call multiple agents for comparison with context
                    agent_responses = self._call_multiple_agents(matching_agents, contextual_input)
                    
                    # Summarize and compare responses
                    comparison_summary = self._summarize_comparison_responses(agent_responses, user_input)
                    
                    # Return comparison details
                    return {
                        "response": comparison_summary,
                        "agents_used": [agent.name for agent in matching_agents],
                        "tools_used": [agent.name for agent in matching_agents],
                        "execution_steps": [
                            {
                                "tool_name": agent.name,
                                "tool_input": f"Comparison query with history context",
                                "observation": agent_responses.get(agent.name, "No response")
                            } for agent in matching_agents
                        ] + [
                            {
                                "tool_name": "comparison_summarizer",
                                "tool_input": "Summarize comparison responses with history",
                                "observation": "Comparison summary generated"
                            }
                        ],
                        "total_steps": len(matching_agents) + 1,
                        "comparison_mode": True
                    }
                else:
                    print(f"Not enough agents found for comparison, falling back to normal routing")
            
            # Normal single-agent processing with context
            result = self.agent_executor.invoke({"input": contextual_input})
            
            # Extract the output
            output = result.get("output", "No response generated")
            
            # Extract execution details
            intermediate_steps = result.get("intermediate_steps", [])
            
            # Analyze intermediate steps to extract agents and tools used
            agents_used = set()
            tools_used = set()
            execution_steps = []
            
            for step in intermediate_steps:
                if len(step) >= 2:
                    action, observation = step[0], step[1]
                    
                    # Extract tool/agent name from action
                    if hasattr(action, 'tool'):
                        tool_name = action.tool
                        tools_used.add(tool_name)
                        
                        # Check if this tool is actually a sub-agent
                        for agent in self.sub_agents:
                            if agent.name == tool_name:
                                agents_used.add(tool_name)
                                break
                        else:
                            # It's a regular tool, not a sub-agent
                            pass
                    
                    # Record execution step
                    execution_steps.append({
                        "tool_name": getattr(action, 'tool', 'unknown'),
                        "tool_input": getattr(action, 'tool_input', ''),
                        "observation": str(observation)
                    })
            
            return {
                "response": output,
                "agents_used": list(agents_used),
                "tools_used": list(tools_used),
                "execution_steps": execution_steps,
                "total_steps": len(intermediate_steps),
                "comparison_mode": False
            }
            
        except Exception as e:
            error_msg = f"Master Agent encountered an error: {str(e)}"
            print(f"Error: {error_msg}")
            return {
                "response": error_msg,
                "agents_used": [],
                "tools_used": [],
                "execution_steps": [],
                "total_steps": 0,
                "error": str(e),
                "comparison_mode": False
            }
    
    def _format_conversation_history(self, history: List[dict]) -> str:
        """
        Format conversation history into a readable context string.
        
        Args:
            history: List of conversation messages
            
        Returns:
            str: Formatted conversation history
        """
        if not history:
            return "No previous conversation history."
        
        formatted_history = []
        for msg in history:
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            agent_name = msg.get('agentName')
            timestamp = msg.get('timestamp', '')
            
            if role == 'user':
                formatted_history.append(f"User: {content}")
            elif role == 'assistant':
                agent_info = f" ({agent_name})" if agent_name else ""
                formatted_history.append(f"Assistant{agent_info}: {content}")
            else:
                formatted_history.append(f"{role}: {content}")
        
        return "\n".join(formatted_history)
    
    def update_sub_agents(self, new_sub_agents: List[BaseTool]):
        """
        Update the sub-agents and recreate the master agent executor.
        This is called when the agents configuration is reloaded.
        
        Args:
            new_sub_agents: Updated list of sub-agents as tools
        """
        print("Updating Master Agent with new sub-agents configuration...")
        self.sub_agents = self._sanitize_sub_agent_tools(new_sub_agents)
        self.agent_executor = self._create_master_agent_executor()
        print(f"Master Agent updated with {len(new_sub_agents)} sub-agents")
    
    def get_agent_info(self) -> dict:
        """
        Get information about the current configuration of agents.
        
        Returns:
            dict: Information about loaded agents
        """
        agent_info = {
            "total_agents": len(self.sub_agents),
            "agents": []
        }
        
        for agent in self.sub_agents:
            agent_info["agents"].append({
                "name": agent.name,
                "description": agent.description
            })
        
        return agent_info
    
    async def process_request_async(self, user_input: str) -> str:
        """
        Async version of process_request for web applications.
        
        Args:
            user_input: The user's query or request
            
        Returns:
            str: The response from the appropriate sub-agent
        """
        # For now, this just calls the sync version
        # In a production environment, you might want to use async LangChain components
        return self.process_request(user_input)


def create_master_agent(sub_agents_as_tools: List[BaseTool]) -> MasterAgent:
    """
    Factory function to create a Master Agent instance.
    
    Args:
        sub_agents_as_tools: List of sub-agents wrapped as BaseTool objects
        
    Returns:
        MasterAgent: Configured master agent instance
    """
    if not sub_agents_as_tools:
        raise ValueError("At least one sub-agent must be provided")
    
    return MasterAgent(sub_agents_as_tools)

async def summarize_conversation_async(messages: List[dict]) -> str:
    """
    Generates a concise Vietnamese summary for a given conversation history.

    Args:
        messages: A list of message dictionaries, e.g., [{"role": "user", "content": "..."}]

    Returns:
        A short Vietnamese string summarizing the conversation.
    """
    try:
        # Format the conversation history for the prompt
        history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "D·ª±a v√†o cu·ªôc h·ªôi tho·∫°i sau, h√£y t·∫°o m·ªôt ti√™u ƒë·ªÅ ng·∫Øn g·ªçn, m√¥ t·∫£ b·∫±ng ti·∫øng Vi·ªát trong v√≤ng 5 t·ª´ ho·∫∑c √≠t h∆°n. Kh√¥ng s·ª≠ d·ª•ng d·∫•u ngo·∫∑c k√©p.\n\n<conversation_history>"),
            ("human", "{history}")
        ])

        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.2)
        
        chain = prompt_template | llm
        
        # Invoke the chain with the conversation history
        response = await chain.ainvoke({"history": history})
        
        # The response from the LLM will be an AIMessage object. We need to get its content.
        summary = response.content.strip().replace('"', '')
        
        return summary
    except Exception as e:
        print(f"Error during conversation summarization: {e}")
        return "Cu·ªôc tr√≤ chuy·ªán m·ªõi" # Fallback title 